\documentclass{article}

\begin{document}
\authors{Julian Bangert, Nickolai Zeldovich, Frans Kaashoek} 

\title{Nail: A practical interface generator for binary formats} \subtitle{-How to avoid hitting
  your security assumptions when dealing with untrusted input-}


 \abstract{We present \textit{Nail},
  an interface generator that allows programmers to safely parse and generate protocols defined by a
  Parser-Expression based grammar. Nail uses a richer set of parser combinators that induce an
  internal representation, obviating the need to write semantic actions. Nail also provides
  solutions parsing common patterns such as length and offset fields within binary formats that are
  hard to process with existing parser generators.}
\section{Background}


\subsection{Problems handling untrusted input}
Code handling un-trusted user inputs is most prone to contain security vulnerabilities. While memory
corruption errors or fatal logic flaws can occur anywhere in a program, bugs in the input handling
code are most likely to be exploitable, because an attacker has total control over the inputs to
that code. 

Common wisdom thus says to take special care when preparing code that deals with untrusted input.
However, with currently prevalent software design methods, it is hard to isolate input-handling code
from the rest of the program, as user input is typically passed through the program and processed in
bits and pieces as a 'shotgun parser'\cite{shotgun-parser}.  

A look at the CVE database or Hacker journal such as \textit{Phrack} shows that such design patterns
and the security problems they cause are rampant. Input processing libraries such as libpng
\footnote{\url{http://www.cvedetails.com/vulnerability-list/vendor_id-7294/Libpng.html} shows 24
remote exploits in the between 2007 and 2013.} or Adobe's PDF and Flash viewers are notoriously
plagued with input bugs. Even simpler formats such as the \textit{zlib} compression library have
historically contained multiple security
flaws\foot{\url{http://www.cvedetails.com/vulnerability-list/vendor_id-72/product_id-1820/GNU-Zlib.html}}.

Besides memory corruption vulnerabilities - against which various mitigations such as static
analysis, dynamic instrumentation and various memory protection mechanisms have been developed -
different implementations of the same format can also differ in their understanding of some edge
cases. The security of many systems relies on different components interpreting the same data
similarly.

Such parser differential vulnerabilities were found in various cryptographic systems, including  iOS
\footnote{The XNU kernel and the user code-signing verifier interpret executable metadata
  differently, so the code signature sees different bytes at a virtual address than the executable
  that runs.}\cite{evaders6} and Android\footnote{Android applications are distributed as .zip
  files. Signatures are verified with a Java program, but the program is extracted with a C program.
The java program interprets all fields as signed, whereas the C program treats them as unsigned,
allowing one to replace files in a signed archive.}
\cite{saurik-masterkey} code signing and even the
X.509 protocol underlying SSL \cite{pki-layer}. Such vulnerabilities occur even if the language,
runtime environment or static analysis guarantee absolute memory safety.

Finally, many protocol implementations are so complicated that even without bugs, they inadvertently
provide a full Turing-complete execution environment. Examples include X86 page tables
\cite{bangert-woot}, ELF symbols and relocations\cite{shapiro-woot}.  
The more powerful an input
protocol is, the more control an attacker gains over a systems state - and power to manipulate a
systems state is also a degree of undeserved trust. In the offensive research community, this has
been generalised into treating a program as a \textit{weird machine}\cite{Bratus-langsec} that
operates on an input, analogous to a virtual machine operating on bytecode.

\subsection{Parser generators}
%%TODO
Luckily, we can avoid all of these problems at once if we have a single, executable specification of
what exactly a programs input is supposed to look like. In fact, definitive grammars have long
been written for all sorts of protocols and are the standard way of describing text-based protocols.
Parser generators can automatically create parsers for such languages, and with common classes of
grammars such as regular expressions or deterministic context free languages, that parser can be
expressed in a computationally less powerful form, e.g. a finite automaton or a push down automaton,
limiting the weird machine problem and enabling very efficient implementation.


Specifications of binary protocols, such as RFCs, also contain grammars, although they are
frequently translated into prose and diagrams as opposed to the traditional Backus-Naur-Form.  

Unfortunately, grammars are seldom used directly to recognise machine-created input. For example,
the security-critical and very well-engineered Kerberos distribution \cite{kerberos} uses parser
generators, but only for handling configuration files. A notable exception is the Mongrel
webserver \footnote{\url{http://mongrel2.org/}} which uses a grammar for HTTP written in the
Ragel \cite{ragel-paper} regular expression language. Mongrel was re-written from scratch multiple
times to achieve better scalability and design, yet the grammar could be  re-used across all iterations\cite{patterson-citation}.

 Parser generators for binary protocols were first introduced by the Hammer\cite{hammer} parser.
 While previous parser generators could also be used to write grammars for binary
 protocols\footnote{Theoretically speaking, the alphabet over which a grammar is an  abstract set,
   so most algorithms work just as well on an alphabet of $\{0,1\}$}, doing so is impractically
 inconvenient. Hammer allows the programmer to specify a grammar in terms of bits and bytes instead
 of characters. Common concerns, such as endianness and bit-packing are transparently hidden by the
 library. Hammer implements grammars as language-integrated parser combinators, an approach
 popularised by Parsec for Haskell\cite{leijen2010parsec}. The parser combinator style (to our
 knowledge, first  described in \cite{burge-recursive}) is a natural way of concisely expressing
 top-down grammars \footnote{For more background on the history of expressing grammars, see Bryan
   Ford's masters thesis\cite{packrat}, which also describes the default parsing algorithm used by Hammer} by composing them from one or multiple sub-parsers. 
  Hammer then constructs a tree of function pointers which
 can be invoked to parse a given input into an abstract syntax tree (AST).
 
Working with generic tree structures is tiresome, so the programmer can bind a \textit{semantic action} to a
sub-parser. Whenever the sub-parser is successfully invoked, the parser executes the semantic action
on the output of the sub-parser and places the result as a leaf node in the syntax tree. Other
parser generators frequently skip the AST entirely, requiring the programmer to specify a semantic
action with every rule in the grammar. 

Semantic actions are one source of vulnerability within automatically generated parsers, as they are
typically implemented in C or another general purpose programming language and perform dangerous
operations such as memory allocation and copying. If there is a bug in the semantic actions, the
attacker is likely able to craft an input that invokes the semantic actions with controllable
inputs, as the inputs have not been fully verified yet.

\subsection{Motivation}
\paragraph{Motivation}
While it is possible to express short transformations on the input entirely as semantic actions
\footnote{This is in fact the design rationale, to perform computation on the fly as the parser
  walks the parse tree.}, more complicated programs usually construct an internal representation,
which contains all relevant information from the input in a format native to the programming
language used. For example, a C programmer ideally wants to deal with structs and NULL-terminated
arrays, whereas a C++ programmer might expect STL containers, a Java programmer interfaces, a
Haskell programmer records and a LISP programmer property lists. 
 The structure of this internal representation usually resembles the structure of the grammar. 

Therefore, a typical programmer has to describe the structure of his input twice or even thrice to
use a parser generator. Once, to write a grammar, another time to write the semantic actions
constructing his favourite intermediate representation. In most languages the programmer also has to
write explicit type definitions, which describe the intermediate object model again. Most
programmers will baulk at this multiplication of efforts and either hand-write a top-down parser
which still eliminates writing the semantic actions at the slight cost of verbosity or resort to
passing user input through their control flow, hopefully remembering to verify each part before
it is accessed. 

A similar problem occurs when the programmer wants to generate output. 
Even though not all programs may use the same format for output as they do on input, different
programs might use the same format for input and output and re-using the same grammar is a good way
to save engineering effort and reduce parser differentials.
Some parser generators, e.g.
Boost.Spirit \footnote{\url{http://www.boost.org/doc/libs/1_55_0/libs/spirit/doc/html/index.html}},
allow the same grammar to be re-used for generating output from the intermediate representation.
However, those generators require a new set of semantic actions to be written, even though the
relationship between grammar and intermediate model was already expressed in the actions for the
parser.

\section{Design}

\paragraph{Overview}
To alleviate these problems, Nail provides a richer set of combinators
\paragraph{Offset fields}
Another problem for parser generators is that binary protocols often contain length and offset
fields. While conventional parsing algorithms can deal with bounded offset fields - a finite
automaton can count a bounded integer, and we can feed the (finite) input multiple times to the
finite automaton. However, this implementation is both time-inefficient - why feed many bytes into
the automaton that will just be skipped - and very cumbersome to express with current parser
generators. Therefore, if languages with offset fields are parsed with parser generators, the only
currently feasible way is to add 'ad-hoc' hacks such as changing the input pointer of the generated
parser on the fly. However, 
\paragraph{Hardened parser}

\paragraph{Hardening the parser}
In addition to reducing the risk of memory corruption vulnerabilities within the generated parser,
Nail also provides mitigations against heap corruption vulnerabilities in other parts of the
program.

Memory corruption exploits (frequently) rely on placing controlled values at known (or controlled)
memory locations. Exploit countermeasures  like Address-Space Layout Randomisation\cite{pax-aslr},
 heap cookies \cite{heapcookies} or data execution prevention try to make this harder for
 an attacker. Attackers have developed techniques such as heap spraying \cite{heapspray} and heap
 feng shui\cite{fengshui} to help bypass those mitigations. 


Nail parsers use two arena allocators\cite{arena}, one for the temporary syntax tree and one for the
result. If Nail were to use the normal system allocator, the attacker could cause two packets to be
parsed at the same time (or, given a sufficiently complicated input, two structure within the same
grammar to be parsed sequentially) such that the (invalid) intermediate results from one parse,  
 If there is a buffer overflow past the
end of one input, it is much harder for the attacker craft a sequence of packets to trick the allocator into
placing a known value next to that input.






\end{document}