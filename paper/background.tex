\section{Background}


% For a long time, it has been possible to write a concise and precise specification of what a data
% format looks like, feed it to a tool and let it generate a parser and potentially a generator for
% that format. However, instead of using these tools, programmers have continued to laboriously
% hand-write bug-prone input/output code. One of the reasons they have done so is that existing tools
% were geared towards parsing text-based programming languages, not for dealing with binary protocols,
% although the underlying algorithms remain the same.  


\subsection{Problems handling untrusted input}
Code handling untrusted user inputs is most prone to contain security vulnerabilities. While memory
corruption errors or fatal logic flaws can occur anywhere in a program, bugs in the input handling
code are most likely to be exploitable, because an attacker has total control over the inputs to
that code. 

Common wisdom thus says to take special care when preparing code that deals with untrusted input.
However, with currently prevalent software design methods, it is hard to isolate input-handling code
from the rest of the program, as user input is typically passed through the program and processed in
bits and pieces as a 'shotgun parser'\cite{shotgun-parser}.  

A look at the CVE database or Hacker journal such as \textit{Phrack} shows that such design patterns
and the security problems they cause are rampant. Input processing libraries such as libpng
\footnote{\url{http://www.cvedetails.com/vulnerability-list/vendor_id-7294/Libpng.html} shows 24
remote exploits in the between 2007 and 2013.} or Adobe's PDF and Flash viewers are notoriously
plagued with input bugs. Even simpler formats such as the \textit{zlib} compression library have
historically contained multiple security
flaws\footnote{\url{http://www.cvedetails.com/vulnerability-list/vendor_id-72/product_id-1820/GNU-Zlib.html}}.

Besides memory corruption vulnerabilities - against which various mitigations such as static
analysis, dynamic instrumentation and various memory protection mechanisms have been developed -
different implementations of the same format can also differ in their understanding of some edge
cases. The security of many systems relies on different components interpreting the same data
similarly.

Such parser differential vulnerabilities were found in various cryptographic systems, including  iOS
\footnote{The XNU kernel and the user code-signing verifier interpret executable metadata
  differently, so the code signature sees different bytes at a virtual address than the executable
  that runs.}\cite{evaders6} and Android\footnote{Android applications are distributed as .zip
  files. Signatures are verified with a Java program, but the program is extracted with a C program.
The java program interprets all fields as signed, whereas the C program treats them as unsigned,
allowing one to replace files in a signed archive.}
\cite{saurik-masterkey} code signing and even the
X.509 protocol underlying SSL \cite{DBLP:conf/fc/KaminskyPS10}. Such vulnerabilities occur even if the language,
runtime environment or static analysis guarantee absolute memory safety.

Finally, many protocol implementations are so complicated that even without bugs, they inadvertently
provide a full Turing-complete execution environment. Examples include X86 page tables
\cite{bangert2013page}, ELF symbols and relocations\cite{shapiro2013weird}.  
The more powerful an input
protocol is, the more control an attacker gains over a systems state - and power to manipulate a
systems state is also a degree of undeserved trust. In the offensive research community, this has
been generalised into treating a program as a \textit{weird machine}\cite{bratus2011exploit} that
operates on an input, analogous to a virtual machine operating on bytecode.

Proper input recognition has been shown to be an excellent way of eliminating malicious inputs. In
one case, a hand-written PDF parser could eliminate $98\%$ of known malicious PDFs\cite{Bogk-PDF}.


\subsection{Parser generators}
%%TODO
Luckily, we can avoid all of these problems at once if we have a single, executable specification of
what exactly a programs input is supposed to look like. In fact, definitive grammars have long
been written for all sorts of protocols and are the standard way of describing text-based protocols.
Parser generators can automatically create parsers for such languages, and with common classes of
grammars such as regular expressions or deterministic context free languages, that parser can be
expressed in a computationally less powerful form, e.g. a finite automaton or a push down automaton,
limiting the weird machine problem and enabling very efficient implementation. \cite{Knuth1965607}


Specifications of binary protocols, such as RFCs, also contain grammars, although they are
frequently translated into prose and diagrams as opposed to the traditional Backus-Naur-Form.  


Unfortunately, grammars are seldom used directly to recognise machine-created input. For example,
the security-critical and very well-engineered MIT Kerberos distribution uses parser
generators, but only for handling configuration files. A notable exception is the Mongrel
web server \footnote{\url{http://mongrel2.org/}} which uses a grammar for HTTP written in the
Ragel \cite{ragel-paper} regular expression language. Mongrel was re-written from scratch multiple
times to achieve better scaleability and design, yet the grammar could be  re-used across all iterations\cite{patterson-citation}.

 Parser generators for binary protocols were first introduced by the Hammer\footnote{\url{http://github.com/upstandinghackers/hammer/}} parser.
 While previous parser generators could also be used to write grammars for binary
 protocols\footnote{Theoretically speaking, the alphabet over which a grammar is an  abstract set,
   so most algorithms work just as well on an alphabet of $\{0,1\}$}, doing so is impractically
 inconvenient. Hammer allows the programmer to specify a grammar in terms of bits and bytes instead
 of characters. Common concerns, such as endianness and bit-packing are transparently hidden by the
 library. Hammer implements grammars as language-integrated parser combinators, an approach
 popularised by Parsec for Haskell\cite{LeijenMeijer:parsec} and. The parser combinator style (to our
 knowledge, first  described in \cite{burge1975recursive}) is a natural way of concisely expressing
 top-down grammars\cite{Danielsson:2010:TPC:1863543.1863585} \footnote{For more background on the history of expressing grammars, see Bryan
   Ford's masters thesis\cite{ford2002packrat}, which also describes the default parsing algorithm used by Hammer} by composing them from one or multiple sub-parsers. 
  Hammer then constructs a tree of function pointers which
 can be invoked to parse a given input into an abstract syntax tree (AST).
 
Working with generic tree structures is tiresome, so the programmer can bind a \textit{semantic action} to a
sub-parser. Whenever the sub-parser is successfully invoked, the parser executes the semantic action
on the output of the sub-parser and places the result as a leaf node in the syntax tree. Other
parser generators frequently skip the AST entirely, requiring the programmer to specify a semantic
action with every rule in the grammar. 

Semantic actions are one source of vulnerability within automatically generated parsers, as they are
typically implemented in C or another general purpose programming language and perform dangerous
operations such as memory allocation and copying. If there is a bug in the semantic actions, the
attacker is likely able to craft an input that invokes the semantic actions with controllable
inputs, as the inputs have not been fully verified yet.

\subsection{Motivation}
\paragraph{Motivation}
While it is possible to express short transformations on the input entirely as semantic actions
\footnote{This is in fact the design rationale, to perform computation on the fly as the parser
  walks the parse tree.}, more complicated programs usually construct an internal representation,
which contains all relevant information from the input in a format native to the programming
language used. For example, a C programmer ideally wants to deal with structs and NULL-terminated
arrays, whereas a C++ programmer might expect STL containers, a Java programmer interfaces, a
Haskell programmer records and a LISP programmer property lists. 
 The structure of this internal representation usually resembles the structure of the grammar. 

Therefore, a typical programmer has to describe the structure of his input twice or even thrice to
use a parser generator. Once, to write a grammar, another time to write the semantic actions
constructing his favourite intermediate representation. In most languages the programmer also has to
write explicit type definitions, which describe the intermediate object model again. Most
programmers will baulk at this multiplication of efforts and either hand-write a top-down parser
which still eliminates writing the semantic actions at the slight cost of verbosity or resort to
passing user input through their control flow, hopefully remembering to verify each part before
it is accessed. 

A similar problem occurs when the programmer wants to generate output. 
Even though not all programs may use the same format for output as they do on input, different
programs might use the same format for input and output and re-using the same grammar is a good way
to save engineering effort and reduce parser differentials.
Some parser generators, e.g.
Boost.Spirit \footnote{\url{http://www.boost.org/doc/libs/1_55_0/libs/spirit/doc/html/index.html}},
allow the same grammar to be re-used for generating output from the intermediate representation.
However, those generators require a new set of semantic actions to be written, even though the
relationship between grammar and intermediate model was already expressed in the actions for the
parser.

